{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6805b2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned.txt\n",
      "Saved words.txt\n",
      "Saved top10words.txt\n",
      "Saved timecompared.txt\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "#  Setup \n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "output_dir = \"ntlk_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read and Clean Text\n",
    "with open(\"alice29.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Keep only letters and normalize to lowercase\n",
    "text = re.sub(r\"[^A-Za-z\\s]\", \" \", text)\n",
    "text = re.sub(r\"\\s+\", \" \", text).strip().lower()\n",
    "\n",
    "# Process and Measure Time\n",
    "start_sent = time.time()\n",
    "sentences = sent_tokenize(text)\n",
    "time_sent = time.time() - start_sent\n",
    "\n",
    "# Measure Word Tokenization\n",
    "start_word = time.time()\n",
    "words = word_tokenize(text)\n",
    "time_word = time.time() - start_word\n",
    "\n",
    "# Filter words (remove stopwords and short words)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "clean_words = [w for w in words if w not in stop_words and len(w) > 2]\n",
    "\n",
    "# Outputs\n",
    "# Save cleaned text\n",
    "with open(f\"{output_dir}/nltk_cleaned.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\" \".join(clean_words))\n",
    "print(\"Saved cleaned.txt\")\n",
    "\n",
    "# Save word list\n",
    "with open(f\"{output_dir}/nltk_words.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(clean_words))\n",
    "print(\"Saved words.txt\")\n",
    "\n",
    "# Save Top 10 words\n",
    "top10 = Counter(clean_words).most_common(10)\n",
    "with open(f\"{output_dir}/nltk_top10words.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Word\\tCount\\n\")\n",
    "    for word, count in top10:\n",
    "        f.write(f\"{word}\\t{count}\\n\")\n",
    "print(\"Saved top10words.txt\")\n",
    "\n",
    "# Save Time Comparison\n",
    "with open(f\"timecompared.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Framework\\tOperation\\tSeconds\\n\")\n",
    "    f.write(f\"NLTK\\tsent_tokenize\\t{time_sent:.6f}\\n\")\n",
    "    f.write(f\"NLTK\\tword_tokenize\\t{time_word:.6f}\\n\")\n",
    "print(\"Saved timecompared.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db30dafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved spacy_cleaned.txt\n",
      "Saved spacy_words.txt\n",
      "Saved spacy_top10words.txt\n",
      "Appended SpaCy performance to timecompared.txt\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from collections import Counter\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Setup\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# Increase max_length for large text\n",
    "nlp.max_length = 2000000\n",
    "\n",
    "output_dir = \"spacy_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read and Clean Text (Same as before)\n",
    "with open(\"alice29.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = re.sub(r\"[^A-Za-z\\s]\", \" \", text)\n",
    "text = re.sub(r\"\\s+\", \" \", text).strip().lower()\n",
    "\n",
    "# Measure Sentence Tokenization\n",
    "nlp_sent = spacy.blank(\"en\")\n",
    "nlp_sent.add_pipe(\"sentencizer\")\n",
    "\n",
    "start_sent = time.time()\n",
    "doc_sent = nlp_sent(text)\n",
    "sentences = list(doc_sent.sents)\n",
    "time_sent = time.time() - start_sent\n",
    "\n",
    "# Measure Word Tokenization\n",
    "start_word = time.time()\n",
    "doc = nlp.make_doc(text) # Tokenizer only\n",
    "words = [token.text for token in doc]\n",
    "time_word = time.time() - start_word\n",
    "\n",
    "# Filter words\n",
    "clean_words = [w for w in words if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "# Outputs\n",
    "\n",
    "# Save cleaned text\n",
    "with open(f\"{output_dir}/spacy_cleaned.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\" \".join(clean_words))\n",
    "print(\"Saved spacy_cleaned.txt\")\n",
    "\n",
    "# Save word list\n",
    "with open(f\"{output_dir}/spacy_words.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(clean_words))\n",
    "print(\"Saved spacy_words.txt\")\n",
    "\n",
    "# Save Top 10 words\n",
    "top10 = Counter(clean_words).most_common(10)\n",
    "with open(f\"{output_dir}/spacy_top10words.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Word\\tCount\\n\")\n",
    "    for word, count in top10:\n",
    "        f.write(f\"{word}\\t{count}\\n\")\n",
    "print(\"Saved spacy_top10words.txt\")\n",
    "\n",
    "# Append Time Comparison to the existing file\n",
    "# Assuming the previous cell created 'ntlk_output/timecompared.txt'\n",
    "compare_file = \"timecompared.txt\"\n",
    "if os.path.exists(compare_file):\n",
    "    with open(compare_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"SpaCy\\tsent_tokenize\\t{time_sent:.6f}\\n\")\n",
    "        f.write(f\"SpaCy\\tword_tokenize\\t{time_word:.6f}\\n\")\n",
    "    print(f\"Appended SpaCy performance to {compare_file}\")\n",
    "else:\n",
    "    with open(f\"timecompared.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"Framework\\tOperation\\tSeconds\\n\")\n",
    "        f.write(f\"SpaCy\\tsent_tokenize\\t{time_sent:.6f}\\n\")\n",
    "        f.write(f\"SpaCy\\tword_tokenize\\t{time_word:.6f}\\n\")\n",
    "    print(f\"Saved timecompared.txt to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "176140bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved textblob_cleaned.txt\n",
      "Saved textblob_words.txt\n",
      "Saved textblob_top10words.txt\n",
      "Appended TextBlob performance to timecompared.txt\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Setup\n",
    "# TextBlob relies on NLTK corpora\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "output_dir = \"textblob_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read and Clean Text\n",
    "with open(\"alice29.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = re.sub(r\"[^A-Za-z\\s]\", \" \", text)\n",
    "text = re.sub(r\"\\s+\", \" \", text).strip().lower()\n",
    "\n",
    "# Measure Sentence Tokenization\n",
    "start_sent = time.time()\n",
    "blob_sent = TextBlob(text)\n",
    "sentences = blob_sent.sentences\n",
    "time_sent = time.time() - start_sent\n",
    "\n",
    "# Measure Word Tokenization\n",
    "start_word = time.time()\n",
    "blob_word = TextBlob(text)\n",
    "words = blob_word.words\n",
    "time_word = time.time() - start_word\n",
    "\n",
    "# Filter words\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "# Convert TextBlob Word objects to string for consistent filtering\n",
    "clean_words = [str(w) for w in words if str(w) not in stop_words and len(w) > 2]\n",
    "\n",
    "# Outputs\n",
    "\n",
    "# Save cleaned text\n",
    "with open(f\"{output_dir}/textblob_cleaned.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\" \".join(clean_words))\n",
    "print(\"Saved textblob_cleaned.txt\")\n",
    "\n",
    "# Save word list\n",
    "with open(f\"{output_dir}/textblob_words.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(clean_words))\n",
    "print(\"Saved textblob_words.txt\")\n",
    "\n",
    "# Save Top 10 words\n",
    "top10 = Counter(clean_words).most_common(10)\n",
    "with open(f\"{output_dir}/textblob_top10words.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Word\\tCount\\n\")\n",
    "    for word, count in top10:\n",
    "        f.write(f\"{word}\\t{count}\\n\")\n",
    "print(\"Saved textblob_top10words.txt\")\n",
    "\n",
    "# Append Time Comparison\n",
    "compare_file = \"timecompared.txt\"\n",
    "if os.path.exists(compare_file):\n",
    "    with open(compare_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"TextBlob\\tsent_tokenize\\t{time_sent:.6f}\\n\")\n",
    "        f.write(f\"TextBlob\\tword_tokenize\\t{time_word:.6f}\\n\")\n",
    "    print(f\"Appended TextBlob performance to {compare_file}\")\n",
    "else:\n",
    "    with open(f\"timecompared.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"Framework\\tOperation\\tSeconds\\n\")\n",
    "        f.write(f\"TextBlob\\tsent_tokenize\\t{time_sent:.6f}\\n\")\n",
    "        f.write(f\"TextBlob\\tword_tokenize\\t{time_word:.6f}\\n\")\n",
    "    print(f\"Saved timecompared.txt to {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
